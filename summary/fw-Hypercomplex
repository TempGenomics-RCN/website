## Background

The hypercomplex numbers were introduced by Marius Sophus Lie, a Norwegian
mathematician. They have applications to geometry, and perhaps more
surprisingly, to other fields as well, including calculus.

Our goal in exploring the world of hypercomplex numbers is to:

- understand the complex numbers as an algebra over the reals
- list and understand the basic desirable properties of complex numbers that can
  be generalized
- find generalizations of complex numbers in two dimensions
- explore the properties, including the geometry, of these generalizations
- explore possible further generalizations

We will use the references listed below for this talk:

- Catoni, F., Cannata, R., Catoni, V., & Zampetti, P. (2004). Two-dimensional
  hypercomplex numbers and related trigonometries and geometries. *Advances in
  Applied Clifford Algebras*, 14(1), 47-68. doi:10.1007/s00006-004-0008-2

## The Definition of the Complex Number Field ``\mathbf{C}``

For this talk, we will primarily work within ``\mathbf{R}^2``. We will first
focus our attention on linear transformations of two types: rotations and
dilations. A rotation is a distance-preserving transformation that only changes
angle, and a dilation is a simple transformation that uniformly scales all
distances. In particular, we allow the projection of everything onto the origin
to be a dilation, and we allow reflections across the origin also.

We note that all dilations can be expressed as

```math
tI = \begin{bmatrix} t & 0 \\ 0 & t \end{bmatrix}
```

for ``t∈\mathbf{R}`` a free parameter.

We note that all rotations can be expressed as the orthogonal matrices

```math
R = \begin{bmatrix} \cos θ & -\sin θ \\ \sin θ & \cos θ \end{bmatrix}
```

for ``θ∈\mathbf{R}`` a free parameter.

Let a rotation-dilation be a linear transformation that rotates an object, then
dilates it. Then all rotation-dilations are of the form

```math
tR = tIR = \begin{bmatrix} t & 0 \\ 0 & t \end{bmatrix}
\begin{bmatrix} \cos θ & -\sin θ \\ \sin θ & \cos θ \end{bmatrix}
= \begin{bmatrix} t\cos θ & -t\sin θ \\ t\sin θ & t\cos θ \end{bmatrix}
```

for ``t∈\mathbf{R}`` and ``θ∈\mathbf{R}`` free parameters.

By making the substitution ``a = t\cos θ`` and ``b = t\sin θ``, we can express
this as

```math
tR = \begin{bmatrix} a & -b \\ b & a \end{bmatrix}
```

We now have a characterization of all linear transformations that either dilate
or rotate the two-dimensional plane. The operation of matrix multiplication
represents the composition of these linear transformations. Thus we have defined
an algebra which allows us to describe all of these kinds of transformations,
and the relationship between them. This algebra of dilations and rotations, as
some of you might recognize, has a name. It's the algebra of complex numbers.
We define

```math
\mathbf{C} = \operatorname{span}\left(\begin{bmatrix}
  1 & 0 \\ 0 & 1
\end{bmatrix}, \begin{bmatrix}
  0 & -1 \\ 1 & 0
\end{bmatrix}\right)
```

and for ease of notation, we will denote by ``1_\mathbf{C}`` (or simply ``1``)
the matrix ``\begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}``, and by
``\mathrm{i}`` the matrix ``\begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix}``.
Let us investigate some properties of this algebra.

### Properties

We inherit certain properties of complex numbers directly from the definition:

- ``\mathbf{C}`` is a two-dimensional vector space over ``\mathbf{R}``
- multiplication distributes over addition
- multiplication is associative
- there is a multiplicative identity, and it's ``1_\mathbf{C}``
- there is an additive identity, and it's ``0_\mathbf{C}``
- every element has an additive inverse

It turns out that we can recover the other properties of the complex numbers
easily from this definition.

Dilation matrices commute with all other matrices because they are multiples of
the identity. And rotation matrices commute with themselves, because the
composition of two rotation matrices is simply the addition of angles. So all
matrices of the form ``tR`` commute with each other. Therefore, ``\mathbf{C}``
is a commutative algebra.

Let ``z=a1_\mathbf{C}+b\mathrm{i}∈\mathbf{C}`` and define

```math
\|z\| = \det z = a^2 + b^2
```

(note that this is the norm usually used in algebra; the norm used in analysis
is the square root) and from the multiplicative property of the determinant, we
see that this norm is multiplicative; i.e. if ``w∈\mathbf{C}`` and
``z∈\mathbf{C}``, then

```math
\|wz\| = \det (wz) = \det w \det z
```

Let us now define an involution, called the **conjugate**:

```math
\overline{z} = z^t = a1_\mathbf{C} - b\mathrm{i}
```

and note the following familiar results:

- ``\overline{\overline{z}} = z``
- ``\overline{tz + w} = t\overline{z} + \overline{w}``
- ``\overline{zw} = \overline{w}\overline{z} = \overline{z}\overline{w}``
- ``\|\overline{z}\| = \|z\|``

inherited from known properties of the transpose. We can also note there is a
natural embedding of the real numbers within this complex field; they correspond
to exactly the self-conjugate complex numbers; i.e. the symmetric matrices.

Note that, if ``z = a1_\mathbf{C} + b\mathrm{i}``, then

```math
\overline{z}z = z^tz = \begin{bmatrix}
  a & b \\ -b & a
\end{bmatrix} \begin{bmatrix}
  a & -b \\ b & a
\end{bmatrix} = \begin{bmatrix}
  a^2 & 0 \\ 0 & b^2
\end{bmatrix} = (a^2 + b^2)1_\mathbf{C}
```

which, interestingly, means that if ``a^2 + b^2 \ne 0``, then

```math
\frac{\overline{z}}{\|z\|} z = 1_\mathbf{C}
```

so if ``z \ne 0_\mathbf{C}``, then there is ``z^{-1}``, an inverse for ``z``. We
have, here, verified all the field axioms (in a somewhat disorganized fashion).
So ``\mathbf{C}`` is a field.

You may already be familiar with the geometry of complex numbers. After all,
it's the standard Euclidean geometry that we see every day. The unit circle on
the complex numbers is the familiar unit circle:

![Unit circle, ``x^2 + y^2 = 1``, of complex numbers](/images/circle.png)

## Unital Algebra

Let us define formally an **algebra** ``A`` over a field ``\mathbf{R}`` as a
vector space ``A`` over ``\mathbf{R}`` with an additional bilinear binary
operation, multiplication ``A × A \to A``. The requirement that the operation is
bilinear can be restated as:

We require that this binary operation is compatible with the scalar
multiplication; that is, if ``s∈\mathbf{R}``, ``t∈\mathbf{R}`` and ``x∈A``,
``y∈A``, then ``(sx)(ty) = (st)(xy)``.

We also require that the binary operation distributes over regular addition of
vectors; if ``x∈A``, ``y∈A``, and ``z∈A``, then

- ``x(y+z) = xy + xy``
- ``(x+y)z = xz + yz``

Let us define an **associative algebra** to be an algebra, with the further
requirement that the binary multiplication operation is associative; i.e. if
``x∈A``, ``y∈A``, and ``z∈A``, then ``(xy)z`` = ``x(yz)``.

Let us define a **commutative algebra** to be an algebra, with the further
requirement that the binary multiplication operation is commutative; i.e. if
``x∈A`` and ``y∈A``, then ``xy = yx``.

Let us further define a **unital algebra** to be an algebra with a
multiplicative identity ``1∈A`` such that if ``x∈A``, then ``1x=x1=x``.

## Finding Two-Dimensional Algebras

Having defined the complex numbers as a span of two matrices in
``\mathbf{R}^{2×2}``, a natural question should be whether it is possible to
find similar systems with similar features. But it is not the case that any two
matrices can be used for this span. We require an algebra's multiplication
operation to be closed. So if ``\operatorname{span}(\{A, B\})`` is to generate
an algebra, then we require ``\{A^2, AB, BA, B^2\} ⊆ \operatorname{span}(\{A,
B\})``.

Thus we can find more associative algebras over the reals. For the purposes of
this talk, we will further restrict our attention to unital two-dimensional
algebras, because having a multiplicative identity is a really good thing.

Restricting our attention to unital algebras means that, without loss of
generality, we can look for matrices ``B`` so that ``\operatorname{span}\{I,
B\}`` is an algebra. It turns out that these algebras are necessarily
commutative: if ``C = aI + bB ∈ \operatorname{span}\{I, B\}`` and ``D = cI + dB ∈
\operatorname{span}\{I, B\}``, then

```math
CD = (aI + bB)(cI + dB) = (acI + (ad + bc)B + bdB^2) = (cI + dB)(aI + bB) = DC
```

Furthermore, we want to pick a specific value of ``B`` so that ``B^2 ∈
\operatorname{span}\{I, B\}``. There aren't that many choices any more! Let's
look at a select few interesting choices of ``B``.

### The Split Complex Numbers

Earlier, we considered the case where

```math
\mathrm{i} = \begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix}
```

which generated the field of complex numbers. Let's consider the variation

```math
\mathrm{j} = \begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix}
```

and we will denote as

```math
\mathbf{S} = \operatorname{span}\{1, \mathrm{j}\}
```

Copying over the definition of the norm for complex numbers, we get the norm for
split complex numbers. If ``z = a + b\mathrm{j} ∈ \mathbf{S}``, then

```math
\|z\| = \det z = a^2 - b^2
```

Note, however, that the split complex numbers do not form a field, as there are
zero divisors (lots of them!). We do retain that

```math
\overline{z}z = \|z\|
```

as with the complex numbers. But the split complex numbers attain ``\|z\|=0``
whenever ``a = \pm b``, instead of only when ``z = 0``.

Geometrically, the unit circle on the complex numbers, ``\|z\| = 1``, is the
familiar unit circle. On the split-complex numbers, the same unit circle is
actually a hyperbola. It may not be surprising, then, that the geometry of the
split-complex numbers is called a **hyperbolic geometry**.

![Unit circle, ``x^2 - y^2 = 1``, of split-complex numbers](/images/hyperbola.png)

An application of the split complex numbers is for (signed) interval arithmetic.
Let ``a ± b`` be an interval; it represents all numbers in ``[a - b, a + b]``.
This can be used to represent some kind of physical measurement. If we add two
intervals ``a ± b`` and ``c ± d``, we get the result that ``(a + c) ± (b + d)``.
You can verify that this is indeed the smallest possible result.

Simultaneously, we note that with split complex numbers,

```math
a + b\mathrm{j} + c + d\mathrm{j} = (a + c) + (b + d)\mathrm{j}
```

If we multiply an interval ``a ± b`` by a scalar ``t``, then the resulting
interval should be ``ta ± tb`` (presuming that we are working with positive
numbers). Multiplying ``a ± b`` and ``c ± d`` is a little more complicated.
Consider the case where ``a>b>0`` and ``c>d>0`` first. Then we have that
``(a-b)(c-d)`` is the smallest possible value, and ``(a+b)(c+d)`` is the largest
possible value. The midpoint of these is ``ac + bd``, and the radius is ``ad +
bc``.

Simultaneously, we note that with split complex numbers,

```math
(a + b\mathrm{j})(c + d\mathrm{j}) =
\begin{bmatrix} a & b \\ b & a \end{bmatrix}
\begin{bmatrix} c & d \\ d & c \end{bmatrix} =
\begin{bmatrix} ac + bd & ad + bc \\ ad + bc & ac + bd \end{bmatrix}
```

This may seem like needless abstraction, but the use of split-complex numbers
allows us to use tools from linear algebra. For instance, note that
multiplication and addition alone are sufficient to compute any analytic
function. Suppose we wish to find

```math
\begin{align*}
\exp (2 ± 1)
&= \exp (2 + 1\mathrm{j}) \\
&= \exp \begin{bmatrix}
  2 & 1 \\ 1 & 2
\end{bmatrix} \\
&= \sum_{n=0}^∞ \frac{\begin{bmatrix}
  2 & 1 \\ 1 & 2
\end{bmatrix}^n}{n!} \\
&= \sum_{n=0}^∞ \frac{\begin{bmatrix}
  -\frac{\sqrt{2}}{2} & \frac{\sqrt{2}}{2} \\
  \frac{\sqrt{2}}{2} & \frac{\sqrt{2}}{2}
\end{bmatrix}\begin{bmatrix}
  1 & 0 \\ 0 & 3
\end{bmatrix}^n
\begin{bmatrix}
  -\frac{\sqrt{2}}{2} & \frac{\sqrt{2}}{2} \\
  \frac{\sqrt{2}}{2} & \frac{\sqrt{2}}{2}
\end{bmatrix}}{n!} \\
&= \sum_{n=0}^∞ \frac{\begin{bmatrix}
  \frac{3^n + 1}{2} & \frac{3^n - 1}{2} \\
  \frac{3^n - 1}{2} & \frac{3^n + 1}{2}
\end{bmatrix}}{n!} \\
&= \frac{\exp{3} + \exp{1}}{2} + \frac{\exp{3} - \exp{1}}{2} \mathrm{j}
\end{align*}
```

### The Dual Numbers

Let's consider another algebra, this time generated by

```math
\mathrm{ɛ} = \begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix}
```
and we will denote as

```math
\mathbf{D} = \operatorname{span}\{1, \mathrm{ɛ}\}
```

Copying over the definition of the norm for complex numbers, we get the norm for
dual numbers. If ``z = a + b\mathrm{ɛ} ∈ \mathbf{D}``, then

```math
\|z\| = \det z = a^2
```

As with the split complex numbers, there are zero divisors in this algebra; they
are exactly the scalar multiples of ``\mathrm{ɛ}``. So the dual numbers do not
form a field.

The unit circle ``\|z\| = 1`` on the dual numbers is a pair of parallel lines.
The geometry of dual numbers is quite strange, because no vertical line segments
have any length, and the distance between two points is entirely determined by
their horizontal distance. This type of geometry is called a **parabolic
geometry**.

![Unit circle, ``x^2 = 1``, of dual numbers](/images/parallel-lines.png)

Furthermore, unlike either the complex numbers or the split-complex numbers, the
generating element ``\mathrm{ɛ}`` of the dual numbers is itself a zero-divisor.
Indeed,

```math
\mathrm{ɛ}^2 = \begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix}^2 = 0
```

Despite these irregularities, the dual numbers are among the most incredible
number systems, and are used frequently in computational mathematics. Why? Let
``f`` be an analytic function. Then, for any real number ``a``, we know that

```math
f(a + η) = \sum_{n=0}^∞ f^{(n)}(a) \frac{η^n}{n!}
```

Define ``g(η) = f(a + η)`` which is also analytic, and hence can be extended to
the matrices. We now have the remarkable identity:

```math
\begin{align*}
f(a + bɛ)
&= g(ɛ) \\
&= g\left( \begin{bmatrix} 0 & b \\ 0 & 0 \end{bmatrix} \right) \\
&= \sum_{n=0}^∞ f^{(n)}(a)
\frac{{\left(\begin{bmatrix} 0 & b \\ 0 & 0 \end{bmatrix}\right)}^n}{n!}
\\ &= f(a) + bf'(a)ɛ
\end{align*}
```

from which we derive the following corollary, which is the specific case when
``b = 1``:

```math
f'(a)ɛ = f(a + ɛ) - f(a)
```

This result means that every analytic function can have its derivative evaluated
exactly, which is incredibly useful for many tasks of computational mathematics,
optimization, and machine learning.

As an example, let's do a quick problem: Find the derivative of ``x^x``.
This function is analytic on ``(1, ∞)``, so we can use dual numbers. We compute

```math
\begin{align*}
(x + ɛ)^{(x + ɛ)}
&= \exp ((x + ɛ) \log (x + ɛ)) \\
&= \exp \left((x + ɛ) \left(\log x + \frac{ɛ}{x}\right)\right) \\
&= \exp \left(x \log x + ɛ(1 + \log x)\right) \\
&= \exp (x \log x) + ɛ (1 + \log x) \exp {x \log x} \\
&= x^x + ɛ (1 + \log x) \exp {x \log x}
\end{align*}
```

and extracting the ``ɛ`` term,

```math
\frac{d}{dx} x^x = x^x + x^x \log x
```

where every step we took is entirely mechanical and could be done by a computer.

## Uniqueness of Hypercomplex Numbers

Above we looked at three distinct kinds of hypercomplex number systems. But are
they the only kinds? The answer is that they are, up to isomorphism.

### Theorem

Let ``A`` be a two-dimensional unital commutative associative
``\mathbf{R}``-algebra. Then ``A`` is isomorphic to ``\mathbf{C}``, or it is
isomorphic to ``\mathbf{S}``, or it is isomorphic to ``\mathbf{D}``.

**Proof.** Let ``1`` be the multiplicative identity, and let ``x∈A`` be any
vector not parallel to ``1``. Then ``\operatorname{span}\{1, x\}`` forms a
basis. Let ``x^2 = a + bx``. We write

```math
\begin{align*}
  x^2 &= a + bx \\
  x^2 - bx &= a \\
  x^2 - bx + \frac{b}{4} &= a + \frac{b}{4} \\
  {\left(x - \frac{b}{2}\right)}^2 &= a + \frac{b}{4}
\end{align*}
```

Now consider three cases. If ``a + \frac{b}{4} = 0``, then ``x - \frac{b}{2}``
squares to zero, and since it is not real (since ``x`` is not real), therefore
``\{1, x - \frac{b}{2}\}`` is a basis. But then ``A`` is isomorphic to
``\mathbf{D}``.

If ``a + \frac{b}{4} < 0``, then

```math
{\left(\frac{\left(x - \frac{b}{2}\right)}{\sqrt{-a - \frac{b}{4}}}\right)}^2
= -1
```

and by a similar argument as before, ``A`` is isomorphic to ``\mathbf{C}``.

Otherwise, ``a + \frac{b}{4} > 0``, and

```math
{\left(\frac{\left(x - \frac{b}{2}\right)}{\sqrt{a + \frac{b}{4}}}\right)}^2
= 1
```

and hence ``A`` is isomorphic to ``\mathbf{S}``. QED.
